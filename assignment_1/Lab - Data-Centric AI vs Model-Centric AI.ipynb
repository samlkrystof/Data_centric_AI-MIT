{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e5916b2",
   "metadata": {},
   "source": [
    "# Lab: Data-Centric vs Model-Centric approaches\n",
    "\n",
    "This lab gives an introduction to data-centric vs model-centric approaches to machine learning problems, showing how data-centric approaches can outperform purely model-centric approaches.\n",
    "\n",
    "In this lab, we'll build a classifier for product reviews (restricted to the magazine category), like:\n",
    "\n",
    "> Excellent! I look forward to every issue. I had no idea just how much I didn't know.  The letters from the subscribers are educational, too.\n",
    "\n",
    "Label: ⭐️⭐️⭐️⭐️⭐️ (good)\n",
    "\n",
    "> My son waited and waited, it took the 6 weeks to get delivered that they said it would but when it got here he was so dissapointed, it only took him a few minutes to read it.\n",
    "\n",
    "Label: ⭐️ (bad)\n",
    "\n",
    "We'll work with a dataset that has some issues, and we'll see how we can squeeze only so much performance out of the model by being clever about model choice, searching for better hyperparameters, etc. Then, we'll take a look at the data (as any good data scientist should), develop an understanding of the issues, and use simple approaches to improve the data. Finally, we'll see how improving the data can improve results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5874cd",
   "metadata": {},
   "source": [
    "## Installing software\n",
    "\n",
    "For this lab, you'll need to install [scikit-learn](https://scikit-learn.org/) and [pandas](https://pandas.pydata.org/). If you don't have them installed already, you can install them by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e0ee93",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-07-08T08:23:10.943941500Z",
     "start_time": "2023-07-08T08:22:34.352240800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.3.0-cp311-cp311-win_amd64.whl (9.2 MB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.0.3-cp311-cp311-win_amd64.whl (10.6 MB)\n",
      "Collecting numpy>=1.17.3\n",
      "  Using cached numpy-1.25.0-cp311-cp311-win_amd64.whl (15.0 MB)\n",
      "Collecting scipy>=1.5.0\n",
      "  Using cached scipy-1.11.1-cp311-cp311-win_amd64.whl (44.0 MB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.3.1-py3-none-any.whl (301 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kryst\\pycharmprojects\\data_centric_ai-mit\\venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kryst\\pycharmprojects\\data_centric_ai-mit\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, numpy, joblib, scipy, pandas, scikit-learn\n",
      "Successfully installed joblib-1.3.1 numpy-1.25.0 pandas-2.0.3 pytz-2023.3 scikit-learn-1.3.0 scipy-1.11.1 threadpoolctl-3.1.0 tzdata-2023.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "!pip install scikit-learn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dbfce6",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "First, let's load the train/test sets and take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7405df2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T08:23:49.699422400Z",
     "start_time": "2023-07-08T08:23:42.696020700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a633542",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T08:23:49.814586500Z",
     "start_time": "2023-07-08T08:23:49.699422400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                review label\n401  Harper's is a must for anyone into fashion, or...  good\n428                              Love it. Always good.  good\n479  this also has been a favorite magazine of mine...  good\n474                   new yorker magazine is the best!  good\n993  Just not the Bazaar that it was years ago. Wil...   bad",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>401</th>\n      <td>Harper's is a must for anyone into fashion, or...</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>428</th>\n      <td>Love it. Always good.</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>479</th>\n      <td>this also has been a favorite magazine of mine...</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>474</th>\n      <td>new yorker magazine is the best!</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>993</th>\n      <td>Just not the Bazaar that it was years ago. Wil...</td>\n      <td>bad</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('reviews_train.csv')\n",
    "test = pd.read_csv('reviews_test.csv')\n",
    "\n",
    "test.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6446a894",
   "metadata": {},
   "source": [
    "# Training a baseline model\n",
    "\n",
    "There are many approaches for training a sequence classification model for text data. In this lab, we're giving you code that mirrors what you find if you look up [how to train a text classifier](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html), where we'll train an SVM on [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) features (numeric representations of each text field based on word occurrences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26e13e03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T08:24:11.647940200Z",
     "start_time": "2023-07-08T08:24:03.628374800Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afcb7dbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T08:24:13.641533600Z",
     "start_time": "2023-07-08T08:24:13.606973500Z"
    }
   },
   "outputs": [],
   "source": [
    "sgd_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a09cc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T08:24:15.143328200Z",
     "start_time": "2023-07-08T08:24:14.913345500Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = sgd_clf.fit(train['review'], train['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8885717d",
   "metadata": {},
   "source": [
    "## Evaluating model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2850df30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T08:24:18.110098Z",
     "start_time": "2023-07-08T08:24:18.103565Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60677a08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:57:54.479172200Z",
     "start_time": "2023-07-08T13:57:54.462763600Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_func(clf):\n",
    "    pred = clf.predict(test['review'])\n",
    "    acc = metrics.accuracy_score(test['label'], pred)\n",
    "    print(f'Accuracy: {100*acc:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f77729fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:58:02.474812800Z",
     "start_time": "2023-07-08T13:58:02.418609600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.7%\n"
     ]
    }
   ],
   "source": [
    "evaluate_func(sgd_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3880fa",
   "metadata": {},
   "source": [
    "## Trying another model\n",
    "\n",
    "76% accuracy is not great for this binary classification problem. Can you do better with a different model, or by tuning hyperparameters for the SVM trained with SGD?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bdf2c7",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Can you train a more accurate model on the dataset (without changing the dataset)? You might find this [scikit-learn classifier comparison](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) handy, as well as the [documentation for supervised learning in scikit-learn](https://scikit-learn.org/stable/supervised_learning.html).\n",
    "\n",
    "One idea for a model you could try is a [naive Bayes classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html).\n",
    "\n",
    "You could also try experimenting with different values of the model hyperparameters, perhaps tuning them via a [grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). \n",
    "\n",
    "Or you can even try training multiple different models and [ensembling their predictions](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier), a strategy often used to win prediction competitions like Kaggle.\n",
    "\n",
    "**Advanced:** If you want to be more ambitious, you could try an even fancier model, like training a Transformer neural network. If you go with that, you'll want to fine-tune a pre-trained model. This [guide from HuggingFace](https://huggingface.co/docs/transformers/training) may be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.13.1-py3-none-any.whl (486 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "     -------------------------------------- 81.4/81.4 kB 757.4 kB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "     -------------------------------------- 268.8/268.8 kB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kryst\\pycharmprojects\\data_centric_ai-mit\\venv\\lib\\site-packages (from transformers) (1.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kryst\\pycharmprojects\\data_centric_ai-mit\\venv\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kryst\\pycharmprojects\\data_centric_ai-mit\\venv\\lib\\site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2023.6.3-cp311-cp311-win_amd64.whl (268 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp311-cp311-win_amd64.whl (3.5 MB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Using cached safetensors-0.3.1-cp311-cp311-win_amd64.whl (263 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Using cached pyarrow-12.0.1-cp311-cp311-win_amd64.whl (21.5 MB)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\kryst\\pycharmprojects\\data_centric_ai-mit\\venv\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.2.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Using cached fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.8.4-cp311-cp311-win_amd64.whl (317 kB)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kryst\\pycharmprojects\\data_centric_ai-mit\\venv\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting charset-normalizer<4.0,>=2.0\n",
      "  Downloading charset_normalizer-3.2.0-cp311-cp311-win_amd64.whl (96 kB)\n",
      "     ---------------------------------------- 96.6/96.6 kB 5.4 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.4-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.9.2-cp311-cp311-win_amd64.whl (60 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.3-cp311-cp311-win_amd64.whl (32 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kryst\\pycharmprojects\\data_centric_ai-mit\\venv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\kryst\\pycharmprojects\\data_centric_ai-mit\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kryst\\pycharmprojects\\data_centric_ai-mit\\venv\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kryst\\pycharmprojects\\data_centric_ai-mit\\venv\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\kryst\\pycharmprojects\\data_centric_ai-mit\\venv\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kryst\\pycharmprojects\\data_centric_ai-mit\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: tokenizers, safetensors, xxhash, urllib3, typing-extensions, tqdm, regex, pyarrow, multidict, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, async-timeout, yarl, requests, multiprocess, aiosignal, responses, huggingface-hub, aiohttp, transformers, datasets, evaluate\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 certifi-2023.5.7 charset-normalizer-3.2.0 datasets-2.13.1 dill-0.3.6 evaluate-0.4.0 filelock-3.12.2 frozenlist-1.3.3 fsspec-2023.6.0 huggingface-hub-0.16.4 multidict-6.0.4 multiprocess-0.70.14 pyarrow-12.0.1 regex-2023.6.3 requests-2.31.0 responses-0.18.0 safetensors-0.3.1 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.30.2 typing-extensions-4.7.1 urllib3-2.0.3 xxhash-3.2.0 yarl-1.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-08T11:26:56.363937200Z",
     "start_time": "2023-07-08T11:26:17.540852600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca681e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "# YOUR CODE HERE\n",
    "dataset = load_dataset(\"csv\", data_files={ \"train\": \"review_train.csv\", \"test\": \"review_test.csv\"})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"review\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "dataset = dataset.map(tokenize_function, batched=True, remove_columns=\"review\")\n",
    "\n",
    "label_mapping = {'bad': 0, 'good': 1}\n",
    "\n",
    "# Map the labels to numerical values\n",
    "dataset = dataset.map(lambda example: {'label': label_mapping[example['label']]})\n",
    "\n",
    "dataset.rename_column(\"label\", \"labels\")\n",
    "dataset.set_format(\"torch\")\n",
    "\n",
    "train_loader = DataLoader(dataset[\"train\"], shuffle=True, batch_size=16, num_workers=os.cpu_count())\n",
    "test_loader = DataLoader(dataset[\"test\"], batch_size=16, num_workers=os.cpu_count())\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "total_steps = num_epochs * len(train_loader)\n",
    "scheduler = get_scheduler(\"linear\", optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(total_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "# evaluate your model and see if it does better\n",
    "# than the ones we provided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=preds, references=batch[\"labels\"])\n",
    "\n",
    "results = metric.compute()\n",
    "\n",
    "results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "a530ee43",
   "metadata": {},
   "source": [
    "## Taking a closer look at the training data\n",
    "\n",
    "Let's actually take a look at some of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab34483c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:53:29.812316600Z",
     "start_time": "2023-07-08T13:53:29.780488500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              review label\n0  Based on all the negative comments about Taste...  good\n1  I still have not received this.  Obviously I c...   bad\n2  </tr>The magazine is not worth the cost of sub...  good\n3  This magazine is basically ads. Kindve worthle...   bad\n4  The only thing I've recieved, so far, is the b...   bad",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Based on all the negative comments about Taste...</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I still have not received this.  Obviously I c...</td>\n      <td>bad</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;/tr&gt;The magazine is not worth the cost of sub...</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>This magazine is basically ads. Kindve worthle...</td>\n      <td>bad</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The only thing I've recieved, so far, is the b...</td>\n      <td>bad</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c84e2",
   "metadata": {},
   "source": [
    "Zooming in on one particular data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ebf3a84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:53:37.200780100Z",
     "start_time": "2023-07-08T13:53:37.184613900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review': \"Based on all the negative comments about Taste of Home, I will not subscribeto the magazine. In the past it was a great read.\\nSorry it, too, has gone the 'way of the wind'.<br>o-p28pass4 </br>\", 'label': 'good'}\n"
     ]
    }
   ],
   "source": [
    "print(train.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b839b33f",
   "metadata": {},
   "source": [
    "This data point is labeled \"good\", but it's clearly a negative review. Also, it looks like there's some funny HTML stuff at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e608bbc6",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Take a look at some more examples in the dataset. Do you notice any patterns with bad data points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82e43ff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:54:20.234529600Z",
     "start_time": "2023-07-08T13:54:20.214659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 review label\n3559  this truly is the best car magazine i have eve...  good\n5736  No longer meets my satisfaction.\\nWish to canc...  good\n1265  Guess when you rate an article low, you try to...   bad\n1452  </dt>I canceled the subscription because I no ...  good\n2522  A must for the runner! Whether your an amateur...  good\n1295  does not work on windows or windows tablet.  W...  good\n5802  <dt class=\"hdlist1\">Set</dt>I give this as a g...   bad\n3414          Excellent Service, Quality & Price!</div>   bad\n153            </tr>It is for  younger people, I am 70.  good\n5177  </p><dl>THIS IS AN EXCELLENT MAGAZINE!! IT GIV...   bad\n1311  </div>Ordered this product at the end of April...  good\n2169  <p>Ive never received this on my Kindle althou...  good\n1199  My favorite magazine by far ! This is the word...  good\n1410  I haven't received my magazine yet - where is ...   bad\n3790       One of the best resources for the modern age  good\n1490  they don't have nudes any more so whats the po...   bad\n5178  Love the magazine. I ordered from the actual w...  good\n1185                                         Nice ideas   bad\n1007  </TR>It was ordered by accident and cancelled ...  good\n745   I love the service on my magazine subscription...   bad",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3559</th>\n      <td>this truly is the best car magazine i have eve...</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>5736</th>\n      <td>No longer meets my satisfaction.\\nWish to canc...</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>1265</th>\n      <td>Guess when you rate an article low, you try to...</td>\n      <td>bad</td>\n    </tr>\n    <tr>\n      <th>1452</th>\n      <td>&lt;/dt&gt;I canceled the subscription because I no ...</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>2522</th>\n      <td>A must for the runner! Whether your an amateur...</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>1295</th>\n      <td>does not work on windows or windows tablet.  W...</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>5802</th>\n      <td>&lt;dt class=\"hdlist1\"&gt;Set&lt;/dt&gt;I give this as a g...</td>\n      <td>bad</td>\n    </tr>\n    <tr>\n      <th>3414</th>\n      <td>Excellent Service, Quality &amp; Price!&lt;/div&gt;</td>\n      <td>bad</td>\n    </tr>\n    <tr>\n      <th>153</th>\n      <td>&lt;/tr&gt;It is for  younger people, I am 70.</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>5177</th>\n      <td>&lt;/p&gt;&lt;dl&gt;THIS IS AN EXCELLENT MAGAZINE!! IT GIV...</td>\n      <td>bad</td>\n    </tr>\n    <tr>\n      <th>1311</th>\n      <td>&lt;/div&gt;Ordered this product at the end of April...</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>2169</th>\n      <td>&lt;p&gt;Ive never received this on my Kindle althou...</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>1199</th>\n      <td>My favorite magazine by far ! This is the word...</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>1410</th>\n      <td>I haven't received my magazine yet - where is ...</td>\n      <td>bad</td>\n    </tr>\n    <tr>\n      <th>3790</th>\n      <td>One of the best resources for the modern age</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>1490</th>\n      <td>they don't have nudes any more so whats the po...</td>\n      <td>bad</td>\n    </tr>\n    <tr>\n      <th>5178</th>\n      <td>Love the magazine. I ordered from the actual w...</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>1185</th>\n      <td>Nice ideas</td>\n      <td>bad</td>\n    </tr>\n    <tr>\n      <th>1007</th>\n      <td>&lt;/TR&gt;It was ordered by accident and cancelled ...</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>745</th>\n      <td>I love the service on my magazine subscription...</td>\n      <td>bad</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "train.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae38448f",
   "metadata": {},
   "source": [
    "## Issues in the data\n",
    "\n",
    "It looks like there's some funny HTML tags in our dataset, and those datapoints have nonsense labels. Maybe this dataset was collected by scraping the internet, and the HTML wasn't quite parsed correctly in all cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664845e4",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "To address this, a simple approach we might try is to throw out the bad data points, and train our model on only the \"clean\" data.\n",
    "\n",
    "Come up with a simple heuristic to identify data points containing HTML, and filter out the bad data points to create a cleaned training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5990cdcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:57:20.524354400Z",
     "start_time": "2023-07-08T13:57:20.504815700Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_bad_data(review: str) -> bool:\n",
    "    # YOUR CODE HERE\n",
    "    return '<' in review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092849c1",
   "metadata": {},
   "source": [
    "## Creating the cleaned training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9c7671e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:57:23.552629600Z",
     "start_time": "2023-07-08T13:57:23.536566800Z"
    }
   },
   "outputs": [],
   "source": [
    "train_clean = train[~train['review'].map(is_bad_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1740bf3b",
   "metadata": {},
   "source": [
    "## Evaluating a model trained on the clean training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e83abad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:57:25.958324400Z",
     "start_time": "2023-07-08T13:57:25.942243900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5e3c7d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:57:28.895843400Z",
     "start_time": "2023-07-08T13:57:28.887472800Z"
    }
   },
   "outputs": [],
   "source": [
    "sgd_clf_clean = clone(sgd_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f72b0db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:57:31.525637900Z",
     "start_time": "2023-07-08T13:57:31.414519200Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = sgd_clf_clean.fit(train_clean['review'], train_clean['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ebff4",
   "metadata": {},
   "source": [
    "This model should do significantly better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80b78100",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-08T13:58:19.006811700Z",
     "start_time": "2023-07-08T13:58:18.974660300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.1%\n"
     ]
    }
   ],
   "source": [
    "evaluate_func(sgd_clf_clean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
